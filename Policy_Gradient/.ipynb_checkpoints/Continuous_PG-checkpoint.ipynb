{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e413134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReportedly vanilla policy gradient is useful in reactive scenarios but not so great where planning and \\nsearching to proactively find a good strategy! Will move on to TRPO and DDPG\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math as m\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "\"\"\"\n",
    "Reportedly vanilla policy gradient is useful in reactive scenarios but not so great where planning and \n",
    "searching to proactively find a good strategy! Will move on to TRPO and DDPG\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a9f81588",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize environment\n",
    "\"\"\"\n",
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "action_shape = env.action_space.shape\n",
    "state_shape = env.observation_space.shape\n",
    "action_shape_high = env.action_space.low[0]\n",
    "action_space_low = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3697de17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Continuous_PG:\n",
    "    def __init__(self, model, env, gamma=0.99):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.pi = tf.constant(m.pi)\n",
    "\n",
    "\n",
    "        \n",
    "    def discount_and_normalize_rewards(self, episode_rewards):\n",
    "        # Array for storing the episode rewards\n",
    "        discounted_rewards = np.zeros_like(episode_rewards, dtype=np.float32)\n",
    "        # compute the discounted reward\n",
    "        reward_to_go = 0.0\n",
    "        for i in reversed(range(len(episode_rewards))):\n",
    "            reward_to_go = reward_to_go*self.gamma + episode_rewards[i]\n",
    "            discounted_rewards[i] = reward_to_go\n",
    "        #Normalize and return\n",
    "        discounted_rewards -= np.mean(discounted_rewards) # Baseline\n",
    "        discounted_rewards /= np.std(discounted_rewards) # Varience reduction\n",
    "\n",
    "        return discounted_rewards\n",
    "    \n",
    "    def get_dist_params(self, net_outs):\n",
    "        mu = net_outs * 2\n",
    "        return mu\n",
    "    \n",
    "    def gaussian_policy(self, mu, actions, sig):\n",
    "        # A function that takes the actions taken and returns their probability based on mu\n",
    "        # as returned by the neural network\n",
    "        x = (1 / (sig * tf.math.sqrt(2 * self.pi))) * tf.math.exp(-1 * (1 / 2) * ((actions - mu)/sig) ** 2)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, states, actions, discounted_rewards, sig):\n",
    "        states_tf = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        discounted_rewards_tf = tf.convert_to_tensor(discounted_rewards, dtype=tf.float32)\n",
    "        # Returns the mu for the distribution that we will sample from, want it to be between \n",
    "        # env.action_space.high and env.action_space.low\n",
    "        net_outputs = self.model(states_tf)\n",
    "        #print(\"net_output: \",net_outputs)\n",
    "        mu=2 * net_outputs\n",
    "        # get the policy pi(a,s)\n",
    "        # mu, sig = self.get_dist_params(net_outputs)\n",
    "        pi_a_s = self.gaussian_policy(mu=mu, actions=actions, sig=sig)\n",
    "        #print(\"pi(a|s): \", pi_a_s)\n",
    "        neg_log_probability = -1*tf.math.log(pi_a_s)\n",
    "        loss = tf.reduce_sum(tf.squeeze(neg_log_probability)*discounted_rewards_tf)\n",
    "        return loss\n",
    "    \n",
    "    def train_step(self, states, actions, discounted_rewards, sig, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss(states, actions, discounted_rewards, sig)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7b91582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initiate model\n",
    "\"\"\"\n",
    "\n",
    "input_layer = Input(shape=state_shape)\n",
    "norm = LayerNormalization()(input_layer)\n",
    "layer_1 = Dense(64, activation='relu')(input_layer)\n",
    "layer_2 = Dense(64, activation='relu')(layer_1)\n",
    "output_layer = Dense(1, activation='tanh')(layer_2)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "75452df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CPG = Continuous_PG(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2a04479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_episode(model, env, sig):\n",
    "    episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "    done=False\n",
    "    Return=0\n",
    "    state=env.reset()\n",
    "    while not done:\n",
    "        episode_states.append(list(state))\n",
    "        state = state.reshape((1, env.observation_space.shape[0]))\n",
    "        net_out = CPG.model(state)\n",
    "        mu = CPG.get_dist_params(net_outs=net_out)\n",
    "        a = tf.clip_by_value(np.random.normal(loc=mu, scale=sig),-1.999,1.999)\n",
    "        episode_actions.append(a.numpy()[0])\n",
    "        next_state, reward, done, info = env.step(a.numpy()[0])\n",
    "        Return+=reward\n",
    "        episode_rewards.append(reward)\n",
    "        state=next_state\n",
    "    rewards_to_go = CPG.discount_and_normalize_rewards(episode_rewards)\n",
    "    return episode_states, episode_actions, rewards_to_go, Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664ab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:0, Return:-1520.4954748546463\n",
      "Iteration:25, Return:-1571.6567987125802\n",
      "Iteration:50, Return:-1140.9215176142407\n",
      "Iteration:75, Return:-1637.7448655121968\n",
      "Iteration:100, Return:-1036.1686256357902\n",
      "Iteration:125, Return:-1543.2439757826637\n",
      "Iteration:150, Return:-1636.0142931813575\n",
      "Iteration:175, Return:-1626.8055822362767\n",
      "Iteration:200, Return:-946.3694036327017\n",
      "Iteration:225, Return:-1166.0383442437014\n",
      "Iteration:250, Return:-1417.8113866984213\n",
      "Iteration:275, Return:-1559.0014484046433\n",
      "Iteration:300, Return:-1521.6987207673537\n",
      "Iteration:325, Return:-907.8618395751629\n",
      "Iteration:350, Return:-1526.784841263133\n",
      "Iteration:375, Return:-1511.372840144932\n",
      "Iteration:400, Return:-1523.9947338487007\n",
      "Iteration:425, Return:-1530.9345001294753\n",
      "Iteration:450, Return:-1538.5248437997325\n",
      "Iteration:475, Return:-1444.1549811481511\n",
      "Iteration:500, Return:-1637.767766178241\n",
      "Iteration:525, Return:-1524.4148786575456\n",
      "Iteration:550, Return:-949.9786150061947\n",
      "Iteration:575, Return:-1641.973563421586\n",
      "Iteration:600, Return:-1601.6608230867453\n",
      "Iteration:625, Return:-1539.0204136398863\n",
      "Iteration:650, Return:-1365.283705873602\n",
      "Iteration:675, Return:-925.5933071606906\n",
      "Iteration:700, Return:-1644.7590366538525\n",
      "Iteration:725, Return:-1308.4075815713604\n",
      "Iteration:750, Return:-1196.1275017918188\n",
      "Iteration:775, Return:-941.2073981232517\n",
      "Iteration:800, Return:-1536.6780135252343\n",
      "Iteration:825, Return:-1331.424820838208\n",
      "Iteration:850, Return:-1279.7032452868038\n",
      "Iteration:875, Return:-1439.7590497970396\n",
      "Iteration:900, Return:-977.8729147191067\n",
      "Iteration:925, Return:-1538.8442736919333\n",
      "Iteration:950, Return:-1644.3188968279358\n",
      "Iteration:975, Return:-1513.1637868374305\n",
      "Iteration:1000, Return:-1529.5572045045972\n",
      "Iteration:1025, Return:-1541.3982372185737\n",
      "Iteration:1050, Return:-1632.639671935232\n",
      "Iteration:1075, Return:-1631.3394641307484\n",
      "Iteration:1100, Return:-1502.003771540544\n",
      "Iteration:1125, Return:-1645.3878207169685\n",
      "Iteration:1150, Return:-1634.3536791133665\n",
      "Iteration:1175, Return:-1328.3224706056894\n",
      "Iteration:1200, Return:-1364.5556418272013\n",
      "Iteration:1225, Return:-1437.5224961140486\n",
      "Iteration:1250, Return:-1575.8748346629009\n",
      "Iteration:1275, Return:-972.6434474137618\n",
      "Iteration:1300, Return:-1527.595481343551\n",
      "Iteration:1325, Return:-1503.8503445164736\n",
      "Iteration:1350, Return:-1249.5633248675847\n",
      "Iteration:1375, Return:-1445.2293574765818\n",
      "Iteration:1400, Return:-1260.8055661219428\n",
      "Iteration:1425, Return:-1308.824437253825\n",
      "Iteration:1450, Return:-1518.6456529873064\n",
      "Iteration:1475, Return:-1472.9835724658356\n",
      "Iteration:1500, Return:-1596.779118766559\n",
      "Iteration:1525, Return:-1625.1069224783907\n",
      "Iteration:1550, Return:-1549.1194279262177\n",
      "Iteration:1575, Return:-1435.8057549081925\n",
      "Iteration:1600, Return:-947.3477688277601\n",
      "Iteration:1625, Return:-1149.4946502744533\n",
      "Iteration:1650, Return:-1542.7712602438423\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 10000\n",
    "return_array = np.zeros(num_iterations)\n",
    "returns_benchmark=-5000\n",
    "sig=0.3\n",
    "decay=0.9995\n",
    "for i in range(num_iterations):\n",
    "    sigma = max(sig*decay, 0.05)\n",
    "    episode_states, episode_actions, rewards_to_go, Return= single_episode(CPG.model, env, sig)\n",
    "    return_array[i]=Return\n",
    "    CPG.train_step(episode_states, episode_actions, rewards_to_go, sigma, optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
    "    if Return > returns_benchmark:\n",
    "        best_model=CPG.model\n",
    "    if i%25==0:\n",
    "        print(\"Iteration:{}, Return:{}\".format(i, Return))\n",
    "#         CPG.model.save(\"CPGAgent-v0-episode{}\".format(i))\n",
    "best_model.save(\"CPGAgent-v0-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c632c7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65606f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
