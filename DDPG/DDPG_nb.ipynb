{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e2d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "import gym\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    \n",
    "    def __init__(self, n_actions, state_shape, buffer_size=10000, learning_batch_size=64):\n",
    "        \n",
    "        self.num_actions = n_actions\n",
    "        self.state_size = state_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = learning_batch_size\n",
    "        self.init_agents()\n",
    "    \"\"\"\n",
    "    1. Initialize Main Actor and critic networks\n",
    "    \"\"\"\n",
    "    # Assume we are using Pendulum-v0\n",
    "    def init_agents(self, act_struct = [32,32], crit_struct=[128,64]):\n",
    "        # Actor Network\n",
    "        # ctor takes the state as input and outputs num_actions. Here tanh suits as it is between -1 and 1\n",
    "        # so scales well to the specific proiblem\n",
    "        input_actor = Input(self.state_size)\n",
    "        layer_1_actor = Dense(act_struct[0], activation=\"relu\")(input_actor)\n",
    "        layer_2_actor = Dense(act_struct[1], activation=\"relu\")(layer_1_actor)\n",
    "        tanh_layer = Dense(self.num_actions, activation='tanh')(layer_2_actor)\n",
    "\n",
    "        self.main_actor = Model(inputs=input_actor, outputs=tanh_layer)\n",
    "\n",
    "        # Critic Network\n",
    "        input_critic = Input((self.num_actions + self.state_size))\n",
    "        layer_1_critic = Dense(crit_struct[0], activation=\"relu\")(input_critic)\n",
    "        norm_c2 = LayerNormalization()(layer_1_critic)\n",
    "        layer_2_critic = Dense(crit_struct[1], activation=\"relu\")(norm_c2)\n",
    "        norm_c3 = LayerNormalization()(layer_2_critic)\n",
    "        output_critic = Dense(1, activation=None)(norm_c3)\n",
    "\n",
    "        self.main_critic = Model(inputs=input_critic, outputs=output_critic)\n",
    "\n",
    "        # Copy these networks to initialize the target networks\n",
    "        self.target_actor = tf.keras.models.clone_model(main_actor)\n",
    "        self.target_critic = tf.keras.models.clone_model(main_critic)\n",
    "        # When cloning it seems weights a re initialized randomly so we need to set the weights of the target networks\n",
    "        # those of the main networks\n",
    "        self.target_actor.set_weights(main_actor.get_weights())\n",
    "        self.target_critic.set_weights(main_critic.get_weights())\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Define Utility functions for obtaining the Q value target, loss, the objective func for the actor, the soft \n",
    "    updates and storing and sampling from the buffer.\n",
    "    \"\"\"\n",
    "    def get_critic_target(self, next_states, rewards, dones, gamma=0.99):\n",
    "        # at this stage we are using a batch of states so states should have shape (batch_size, state_shape[0])\n",
    "        # Compute the target networks prediction for next states i.e a' = mu(s')\n",
    "        actions_by_target = self.target_actor(next_states)\n",
    "        # Calculate target critics estimation of the value of s', a'\n",
    "        # First we concatenate the next_states and actions\n",
    "        inputs_to_val_net = tf.concat([next_states, actions_by_target], axis=1)\n",
    "        targ_Q_vals = self.target_critic(inputs_to_val_net)\n",
    "        # Use target critic estimation of Q value for a' and s' to build bellman target\n",
    "        y = rewards + (1-dones) * gamma * targ_Q_vals\n",
    "        return y\n",
    "\n",
    "    def get_critic_loss(self, targets, states):\n",
    "        # To gt the bellman error we now use main actor to find a=mu(s)\n",
    "        actions_by_main = self.main_actor(states)\n",
    "         # Input a=mu(s) and s into the critic network\n",
    "        inputs_to_val_net = tf.concat([states, actions], axis=1)\n",
    "        # Get the main critic estimation of the current state and action\n",
    "        predicted_Q_vals = self.main_critic(inputs_to_val_net)\n",
    "        # calculate the MSE between the predicted q value and the bellman target\n",
    "        mean_squared_errors = tf.keras.losses.MSE(tf.squeeze(targets), tf.squeeze(predicted_Q_vals))\n",
    "        return mean_squared_errors\n",
    "\n",
    "    def actor_objective(self, states):\n",
    "        # The actor objective here should calculate the Q value the whole way from actor through the critic so we can\n",
    "        # get Q(s, mu(s)), which we the find gradients wrt to actor parameters and perform gradient ascent step \n",
    "        actions = self.main_actor(states)\n",
    "        q_val_net_inputs = tf.concat([states, actions], axis=1)\n",
    "        Q_vals = self.main_critic(q_val_net_inputs)\n",
    "        # Want to carry out a gradient ascent step here so include -1, since optimizer.apply exclusively implements \n",
    "        # gradient descent\n",
    "        loss = -1 * tf.reduce_mean(Q_vals)\n",
    "        return loss\n",
    "\n",
    "    def train_critic(self, states, rewards, next_states, dones, optimizer=Adam(lr=0.001)):\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        done = tf.convert_to_tensor(dones, dtype=tf.bool)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_targets = self.get_critic_target(next_states,rewards,dones)\n",
    "            loss = self.get_critic_loss(Q_targets, states)\n",
    "        grads = tape.gradient(loss, self.critic_net.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.critic_net.trainable_variables))\n",
    "\n",
    "    def train_actor(self, states, optimizer=Adam(lr=0.0001)):\n",
    "        states = tf.convert_to_tensor(states, tf.float32)\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_vals = self.actor_objective(states)\n",
    "        grads = tape.gradient(Q_vals, self.actor_net.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.actor_net.trainable_variables))\n",
    "\n",
    "\n",
    "    def soft_update(self, tau=0.005):\n",
    "        # implement the soft target update by list wise computing the update\n",
    "        actor_target_weights = self.target_actor.get_weights()\n",
    "        actor_main_weights = self.main_actor.get_weights()\n",
    "        critic_target_weights = self.target_critic.get_weights()\n",
    "        critic_main_weights = self.main_critic.get_weights()\n",
    "\n",
    "        new_actor_target_weights = []\n",
    "        for t in range(len(actor_target_weights)):\n",
    "            new_actor_target_weights.append(tau * actor_main_weights[t] + (1 - tau) * actor_target_weights[t])\n",
    "        self.target_actor.set_weights(new_target_weights)\n",
    "        \n",
    "        new_critic_target_weights = []\n",
    "        for t in range(len(critic_target_weights)):\n",
    "            new_critic_target_weights.append(tau * critic_main_weights[t] + (1 - tau) * critic_target_weights[t])\n",
    "        self.target_critic.set_weights(new_target_weights)\n",
    "\n",
    "\n",
    "    def store_sample(self, state, action, next_state, reward, done):\n",
    "        # Store experience instances in the replay buffer\n",
    "        if len(self.buffer)< self.buffer_size:\n",
    "            self.buffer.append((state, action, next_state, reward, done))\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append((state, action, next_state, reward, done))\n",
    "\n",
    "    def sample_experiences(self, batch_size):\n",
    "        \"\"\"\n",
    "        Returns 5 np.array() objects containing batch_size elements corresponding to the minibatch for \n",
    "        each element recorded in the replay buffer\n",
    "        \"\"\"\n",
    "        states = np.zeros((batch_size, self.state_size), dtype=np.float32)\n",
    "        actions = np.zeros((batch_size, self.num_actions), dtype=np.float32)\n",
    "        rewards = np.zeros((batch_size,1), dtype=np.float32)\n",
    "        next_states = np.zeros((batch_size, self.state_size), dtype=np.float32)\n",
    "        dones = np.zeros((batch_size,1), dtype=bool)\n",
    "        batch = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        \n",
    "        states[:,:] = self.buffer[batch][0][:]\n",
    "        actions[:,0] = self.buffer[batch][1][0]\n",
    "        next_states[:,:] = self.buffer[batch][2][:]\n",
    "        rewards[:,0] = self.buffer[batch][3]\n",
    "        dones[:] = self.buffer[batch][4]\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    \"\"\"\n",
    "    define a scheduler for the noise sigma\n",
    "    \"\"\"\n",
    "\n",
    "    def sigma_scheduler(self, episode, num_episodes, init_sigma=0.1, final_sigma=0.05):\n",
    "        half_life = num_episodes/5\n",
    "        sigma = max(init_sigma*(0.5)**(episode/half_life), final_sigma)\n",
    "        return sigma\n",
    "\n",
    "    \"\"\"\n",
    "    define an random process for exploration\n",
    "    \"\"\"\n",
    "\n",
    "    def get_noisy_output(self, net_outputs, sig):\n",
    "        action = net_outputs[0]\n",
    "        return tf.clip_by_value(action+np.random.normal(loc=0.0, scale=sig, size = action.shape[0]), clip_value_min=-1, \n",
    "                                    clip_value_max=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
