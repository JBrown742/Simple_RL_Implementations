{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17e6e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LayerNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from DickeStateEnv import DickeStateEnv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Build a for the actor and critic agents\n",
    "\"\"\"\n",
    "class MADDPG:\n",
    "    \"\"\"\n",
    "    Define a class that returns the actor and critic (aswell as duplicate targets) for a single \n",
    "    agent within the MADDPG framework. Here we assume each agent will control a single parameter\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_sizes, gamma=0.95, actor_structs=None, critic_structs=None, n_actions=None,\n",
    "                agent_names=None):\n",
    "        self.gamma = gamma\n",
    "        self.observation_sizes = observation_sizes\n",
    "        self.n_agents = len(observation_sizes)\n",
    "        self.optimizer = optimizer\n",
    "        self.critic_input_size = sum(observation_sizes) + sum(n_actions)\n",
    "        \n",
    "        if agent_names==None:\n",
    "            self._agent_names = ['agent_{}'.format(i) for i in range(self.n_agents)]\n",
    "        else:\n",
    "            self._agent_names = agent_names\n",
    "            \n",
    "        if n_actions==None:\n",
    "            self.n_actions = [1]*self.n_agents\n",
    "        else:\n",
    "            self.n_actions = n_actions\n",
    "            \n",
    "        if actor_structs==None:\n",
    "            self.actor_structs=[[32,32]]*self.n_agents\n",
    "        else:\n",
    "            self.actor_structs=actor_structs\n",
    "        \n",
    "        if critic_structs==None:\n",
    "            self.critic_structs=[[128,64]]*self.n_agents\n",
    "        else:\n",
    "            self.critic_structs=critic_structs\n",
    "        \n",
    "    def build_actor(self, obs_size, struct, num_actions):\n",
    "        \"\"\"\n",
    "        Function for initializing and building the main and target actor networks for a single agent.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        observation_shape: (Tuple) Of type (None, observation_legnth). None here denoted a yet undefined\n",
    "                                   quantity, i.e 1 for single actions, and batch_size when training\n",
    "        num_actions: (int) Number of parameters the agent is responsible for. Default is 1. Each \n",
    "                           agent is responsible for one single control parameter.\n",
    "        hidden_arch: (list) Defines both the number of nodes, and number of hidden layers. Layers are Dense \n",
    "                            by default.\n",
    "                        \n",
    "        Returns\n",
    "        -------\n",
    "        actor: (keras.model object) The network to be used as the actor withing the agent.\n",
    "        actor_: (keras.model object) Identical copy of actor to be used as the agents actor target network.                      \n",
    "        \"\"\"\n",
    "        obs_shape = (1, obs_size)\n",
    "        # observation shape should be (1,obs_size) or (batch_size, obs_size)\n",
    "        actor = Sequential()\n",
    "        for h in struct: \n",
    "            actor.add(Dense(h, activation=\"relu\"))\n",
    "        actor.add(Dense(num_actions, activation=\"sigmoid\"))\n",
    "        actor.build(input_shape=obs_shape)\n",
    "        actor_ = tf.keras.models.clone_model(actor)\n",
    "        actor_.set_weights(actor.get_weights())\n",
    "        return actor, actor_\n",
    "\n",
    "    def build_critic(self, struct):\n",
    "        \"\"\"\n",
    "        Function for initializing and building the main and target critic networks for a single agent.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        observation_shape: (Tuple) Of type (None, observation_legnth). None here denoted a yet undefined\n",
    "                                   quantity, i.e 1 for single actions, and batch_size when training\n",
    "        num_actions: (int) Number of parameters the agent is responsible for. Default is 1. Each \n",
    "                           agent is responsible for one single control parameter.\n",
    "        hidden_arch: (list) Defines both the number of nodes, and number of hidden layers. Layers are Dense \n",
    "                            by default.\n",
    "                        \n",
    "        Returns\n",
    "        -------\n",
    "        critic: (keras.model object) The network to be used as the actor withing the agent.\n",
    "        critic_: (keras.model object) Identical copy of actor to be used as the agents actor target network.                      \n",
    "        \"\"\"\n",
    "    \n",
    "        obs_shape = (1,self.critic_input_size)\n",
    "        critic = Sequential()\n",
    "        for h in struct:\n",
    "            critic.add(Dense(h, activation=\"relu\"))\n",
    "            critic.add(LayerNormalization())\n",
    "        critic.add(Dense(1, activation=None))\n",
    "        critic.build(input_shape=obs_shape)\n",
    "        critic_ = tf.keras.models.clone_model(critic)\n",
    "        critic_.set_weights(critic.get_weights())\n",
    "        return critic, critic_\n",
    "    \n",
    "    def init_agent_networks(self):\n",
    "        self.actors = {}\n",
    "        self.actor_targets = {}\n",
    "        self.critics = {}\n",
    "        self.critic_targets = {}\n",
    "        self.agent_dict = {}\n",
    "        for i, name in enumerate(self.agent_names):\n",
    "            actor, actor_ = self.build_actor(obs_size=self.observation_sizes[i], struct=self.actor_structs[i],\n",
    "                                            num_actions=self.n_actions[i])\n",
    "            critic, critic_ = self.build_critic(struct=self.critic_structs[i])\n",
    "            self.actors[name] = actor\n",
    "            self.actor_targets[name] = actor_\n",
    "            self.critics[name] = critic\n",
    "            self.critic_targets = critic_\n",
    "        return  \n",
    "    \n",
    "    def action_noise(self, action, scale=0.05):\n",
    "        return np.clip((action+np.random.normal(loc=0.0, scale=scale)), a_min=0, a_max=1)\n",
    "    \n",
    "    def get_action_async(self, observation, agent_name, noise=False):\n",
    "        if not noise:\n",
    "            actions = self.actors[agent_name](observations.reshape((1,self.observation_sizes[i]))).numpy()\n",
    "        else:\n",
    "            actor_output = self.actors[agent_name](observations.reshape((1,self.observation_sizes[i])))\n",
    "            action = self.action_noise(actor_output)\n",
    "        return actions\n",
    "    \n",
    "    def get_action_sync(self, observations, noise=False):\n",
    "        actions = []\n",
    "        if not noise:\n",
    "            for i, name in enumerate(self.agent_names):\n",
    "                actions.append(self.actors[name](observations[i].reshape((1,self.observation_sizes[i]))).numpy())\n",
    "        else:\n",
    "            for i, name in enumerate(self.agent_names):\n",
    "                actor_output = self.actors[name](observations[i].reshape((1,self.observation_sizes[i])))\n",
    "                noisy_action = self.action_noise(actor_output)\n",
    "                actions.append(noisy_action)\n",
    "        return actions\n",
    "    \n",
    "    def train_critic(self, index, obs, actions, next_obs, rewards, dones,  optimizer=Adam(lr=0.01)):\n",
    "        # get the critic target and main critic networks corresponding to index\n",
    "        critic_target_net = self.critic_targets[index]\n",
    "        critic_net = self.critics[index]\n",
    "        \n",
    "        next_state = next_obs.copy()\n",
    "        state = obs.copy()\n",
    "        # Initialize a tensor to get the target predicted next actions from next observations\n",
    "        # as well as the main actor predicted current actions from current observations \n",
    "        # to be used in the target and main critic networks\n",
    "        targ_actions = []\n",
    "        for i, actor_targ in enumerate(list(self.actor_targets.values)):\n",
    "            targ_act = actor_targ(next_obs[i])\n",
    "            targ_actions.append(targ_act)\n",
    "            next_state.append(targ_act)\n",
    "            state.append(actions[i])\n",
    "        # First concatenate the list of actor observation arrays into one single array of shape\n",
    "        # (batch_size, obs_len_1 + ..... + obs_len_N), then concatenate all actions to the end of this to \n",
    "        # make a tensor of shape (batch_size, (obs_len_1 + .... + obs_len_N + num_acts_1 + .... + num_acts_N))\n",
    "        concated_next_obs_and_targ_acts = tf.concat(next_state, axis=1)\n",
    "        \n",
    "        concat_obs_and_acts = tf.concat(state, axis=1)\n",
    "        \n",
    "        # Calculate the estimated Q values as per the target critic network on the next observations and actions\n",
    "        estimated_target_Qs = critic_target_net(concated_next_obs_and_targ_acts)\n",
    "        # obtain a batch of bellman targets with shape (batch_size,1)\n",
    "        y = rewards[:,index] + self.gamma * (1-dones[:,index]) * estimated_target_Qs\n",
    "        with tf.GradientTape() as tape: \n",
    "            current_predicted_Qs = critic_net(concat_obs_and_acts)\n",
    "            loss = mse(y,current_predicted_Qs)\n",
    "        grads = tape.gradient(loss, self.critics[index].trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.critics[index].trainable_variables))\n",
    "        \n",
    "    def train_actor(self, index, obs, rewards, dones, optimizer=Adam(lr=0.001)):\n",
    "        # Get a the batch of observations corresponding to agent index\n",
    "        # Note obs will be a list of batches \n",
    "        current_agents_obs = obs[index]\n",
    "        actor = self.actors[index]\n",
    "        critic = self.critics[index]\n",
    "        state = obs.copy()\n",
    "        actions = []\n",
    "        with tf.GradientTape() as tape:\n",
    "            for i, actor in enumerate(list(self.actors.values)):\n",
    "                actor_output = actor(obs[i])\n",
    "                actions.append(actor_output)\n",
    "                state.append(actor_output)\n",
    "            concat_obs_and_acts = tf.concat(state, axis=1)\n",
    "            Q_values = critic(concat_obs_and_acts)\n",
    "            loss = -1 * tf.reduce_mean(Q_values)\n",
    "        grads = tape.gradient(loss, actor.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, actor.trainable_variables))\n",
    "        \n",
    "    def soft_update(self, tau = 0.01):\n",
    "        for agent_idx in self.agent_names:\n",
    "            current_crit_targ_weights = self.critic_targets[agent_idx].get_weights()\n",
    "            current_crit_weights = self.critics[agent_idx].get_weights()\n",
    "            current_act_targ_weights = self.actor_targets[agent_idx].get_weights()   \n",
    "            current_act_weights = self.actors[agent_idx].get_weights()\n",
    "            \n",
    "            new_act_targ_weights = []\n",
    "            for i in range(len(current_act_targ_weights)):\n",
    "                new_act_targ_weights.append(tau * current_act_weights[i] + (1 - tau) * current_act_targ_weights[i])\n",
    "            new_crit_targ_weights = []\n",
    "            for i in range(len(current_crit_targ_weights)):\n",
    "                new_crit_targ_weights.append(tau * current_crit_weights[i] + (1 - tau) * current_crit_targ_weights[i])\n",
    "            self.critic_targets[agent_idx].set_weights(new_crit_targ_weights)\n",
    "            self.actor_targets[agent_idx].set_weights(new_act_targ_weights)\n",
    "    \n",
    "#     def train_critic(self, critic_main, critic_targ, actor_main, actor_targ):\n",
    "        \n",
    "\n",
    "class MultiAgentReplayBuffer:\n",
    "    def __init__(self, max_size, observation_sizes, n_actions, batch_size):\n",
    "        # observations sizes is a list of observation_lengths for each agent ****Careful of ordering\n",
    "        self.n_agents = len(observation_sizes)\n",
    "        # Max memory size \n",
    "        self.mem_size = max_size\n",
    "        # keeps track of how full the memory is\n",
    "        self.mem_cntr = 0\n",
    "        # Batch_size used in the learning process\n",
    "        self.batch_size = batch_size\n",
    "        for i, size in enumerate(observation_sizes):\n",
    "            idx=i+1\n",
    "            self.agent_idx_obs_memory = np.zeros((self.mem_size, size))\n",
    "            self.agent_idx_next_obs_memory = np.zeros((self.mem_size, size))\n",
    "            self.agent_idx_action_memory = np.zeros((self.mem_size, n_actions[i]))\n",
    "        self.reward_memory = np.zeros((self.mem_size, self.n_agents))\n",
    "        self.terminal_memory = np.zeros((self.mem_size, self.n_agents), dtype=bool)\n",
    "        \n",
    "\n",
    "    \n",
    "    def store_transition(self, raw_obs, actions, rewards, next_raw_obs, dones):\n",
    "        # states sould be a list of arrays for each agent, so that the state_memory structure with be\n",
    "        # arrays embedded in a lists embedded in an array mem[elem_idx][agent_idx][obs_elem_idx]\n",
    "        #Â Should turn actions and rewards and dones to lists  also so that each has the same structure\n",
    "        # as the states\n",
    "        \n",
    "        # Each input to this function must be a list\n",
    "        \n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        for i in range(self.n_agents):\n",
    "            idx=int(i+1)\n",
    "            self.agent_idx_obs_memory[index, :] = raw_obs[i][:]\n",
    "            self.agent_idx_next_obs_memory[index, :] =  next_raw_obs[i][:]\n",
    "            self.agent_idx_action_memory[index, :] = np.array(actions[i])[0,:]\n",
    "        self.reward_memory[index, :] = np.array(reward)[:]\n",
    "        self.terminal_memory[index, :] = np.array(dones, dtype=bool)[:]\n",
    "        \n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def sample_buffer(self):\n",
    "        \"\"\"\n",
    "        returns list of batches of observations one for each agent, list of batches of next observations\n",
    "        one for each agent, batch of rewards, batch of actions, batch of done bools\n",
    "        \"\"\"\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        \n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        agent_obs = []\n",
    "        agent_next_obs = []\n",
    "        agent_actions = []\n",
    "        for i in range(self.n_agents):\n",
    "            idx = int(i+1)\n",
    "            agent_idx_obs = self.agent_idx_obs_memory[batch]\n",
    "            agent_obs.append(agent_idx_obs)\n",
    "            agent_idx_next_obs = self.agent_idx_next_obs_memory[batch]\n",
    "            agent_next_obs.append(agent_idx_next_obs)\n",
    "            agent_idx_actions = self.agent_idx_action_memory[batch]\n",
    "            agent_actions.append(agent_idx_actions)\n",
    "        rewards = self.reward_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "        \n",
    "        return agent_obs, agent_next_obs, rewards, agent_actions, terminal\n",
    "    \n",
    "    def ready(self):\n",
    "        if self.mem_cntr>= self.batch_size:\n",
    "            return True\n",
    "        return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
