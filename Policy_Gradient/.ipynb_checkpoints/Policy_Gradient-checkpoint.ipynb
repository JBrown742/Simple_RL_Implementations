{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize environment\n",
    "\"\"\"\n",
    "env = gym.make('CartPole-v1')\n",
    "state_shape = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build Policy Network\n",
    "\"\"\"\n",
    "\n",
    "input_layer = Input(shape=state_shape)\n",
    "layer_1 = Dense(64, activation='relu')(input_layer)\n",
    "layer_2 = Dense(64, activation='relu')(layer_1)\n",
    "layer_3 = Dense(num_actions)(layer_2)\n",
    "output_layer = tf.nn.softmax(layer_3)\n",
    "\n",
    "policy_network = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Def the loss function\n",
    "\"\"\"\n",
    "class REINFORCE:\n",
    "    def __init__(self, model, gamma = 0.98):\n",
    "        self.model=model\n",
    "        self.gamma=gamma\n",
    "    \n",
    "    def loss(self, states, actions, discounted_rewards):\n",
    "        probs = self.model(np.array(states))\n",
    "        mask = tf.one_hot(actions, num_actions)\n",
    "        pi_a_s = tf.reduce_sum(probs * mask, axis=1)\n",
    "        neg_log_policy = -1*tf.math.log(pi_a_s)\n",
    "        loss = tf.reduce_sum(neg_log_policy * discounted_rewards)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, states, actions, discounted_rewards):\n",
    "        optimizer = tf.keras.optimizers.Adam(lr=0.0005)\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss(states, actions, discounted_rewards)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "    \n",
    "    # Define a function for discounting and normalizing rewards, which returns the reward to go array\n",
    "    def discount_and_normalize_rewards(self, episode_rewards):\n",
    "        # Array for storing the episode rewards\n",
    "        discounted_rewards = np.zeros_like(episode_rewards)\n",
    "        # compute the discounted reward\n",
    "        reward_to_go = 0.0\n",
    "        for i in reversed(range(len(episode_rewards))):\n",
    "            reward_to_go = reward_to_go*self.gamma + episode_rewards[i]\n",
    "            discounted_rewards[i] = reward_to_go\n",
    "        #Normalize and return\n",
    "        discounted_rewards -= np.mean(discounted_rewards) # Baseline\n",
    "        discounted_rewards /= np.std(discounted_rewards) # Varience reduction\n",
    "\n",
    "        return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE = REINFORCE(model=policy_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_batch_run(model, env):\n",
    "    episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "    done=False\n",
    "    Return=0\n",
    "    state=env.reset()\n",
    "    while not done:\n",
    "        episode_states.append(state)\n",
    "        state = state.reshape([1,4])\n",
    "        pi = RE.model(state)\n",
    "        a = np.random.choice(np.array([0,1]),p=pi[0].numpy())\n",
    "        episode_actions.append(a)\n",
    "        next_state, reward, done, info = env.step(a)\n",
    "        Return+=reward\n",
    "        episode_rewards.append(reward)\n",
    "        state=next_state\n",
    "    rewards_to_go = RE.discount_and_normalize_rewards(episode_rewards)\n",
    "    return episode_states, episode_actions, rewards_to_go, Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 500\n",
    "return_array = np.zeros(num_iterations)\n",
    "returns_benchmark=0.0\n",
    "for i in range(num_iterations):\n",
    "    returns = np.zeros(8)\n",
    "    episode_states, episode_actions, rewards_to_go, Return= single_batch_run(RE.model, env)\n",
    "    return_array[i]=Return\n",
    "    RE.train_step(episode_states, episode_actions, rewards_to_go)\n",
    "    if Return > returns_benchmark:\n",
    "        best_model=RE.model\n",
    "    if i%50==0:\n",
    "        print(\"Iteration:{}, Return:{}\".format(i, Return))\n",
    "        RE.model.save(\"PGAgent-v2-episode{}\".format(i))\n",
    "best_model.savee(\"PGAgent-v2-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(returns)\n",
    "plt.xlabel(\"Episodes\", fontsize =15)\n",
    "plt.ylabel(\"Return (timestep held upright)\", fontsize=15)\n",
    "plt.title(\"Learning Curve for basic REINFORCE on CartPole-v1\")\n",
    "plt.savefig(\"PGA_learningcurve-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 2\n",
    "batch_size = 8\n",
    "\n",
    "for i in range(num_iterations):\n",
    "   \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-bobby",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-fellowship",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
