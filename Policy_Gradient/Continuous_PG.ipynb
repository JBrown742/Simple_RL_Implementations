{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "199d0efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReportedly vanilla policy gradient is useful in reactive scenarios but not so great where planning and \\nsearching to proactively find a good strategy! Will move on to TRPO and DDPG\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math as m\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "\"\"\"\n",
    "Reportedly vanilla policy gradient is useful in reactive scenarios but not so great where planning and \n",
    "searching to proactively find a good strategy! Will move on to TRPO and DDPG\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0912cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize environment\n",
    "\"\"\"\n",
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "action_shape = env.action_space.shape\n",
    "state_shape = env.observation_space.shape\n",
    "action_shape_high = env.action_space.low[0]\n",
    "action_space_low = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "37b503b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Continuous_PG:\n",
    "    def __init__(self, model, env, gamma=1):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.pi = tf.constant(m.pi)\n",
    "\n",
    "\n",
    "        \n",
    "    def discount_and_normalize_rewards(self, episode_rewards):\n",
    "        # Array for storing the episode rewards\n",
    "        discounted_rewards = np.zeros_like(episode_rewards, dtype=np.float32)\n",
    "        # compute the discounted reward\n",
    "        reward_to_go = 0.0\n",
    "        for i in reversed(range(len(episode_rewards))):\n",
    "            reward_to_go = reward_to_go*self.gamma + episode_rewards[i]\n",
    "            discounted_rewards[i] = reward_to_go\n",
    "        #Normalize and return\n",
    "        discounted_rewards -= np.mean(discounted_rewards) # Baseline\n",
    "        discounted_rewards /= np.std(discounted_rewards) # Varience reduction\n",
    "\n",
    "        return discounted_rewards\n",
    "    \n",
    "    def get_dist_params(self, net_outs):\n",
    "        mu = net_outs * 2\n",
    "        return mu\n",
    "    \n",
    "    def gaussian_policy(self, mu, actions, sig):\n",
    "        # A function that takes the actions taken and returns their probability based on mu\n",
    "        # as returned by the neural network\n",
    "        x = (1 / (sig * tf.math.sqrt(2 * self.pi))) * tf.math.exp(-1 * (1 / 2) * ((actions - mu)/sig) ** 2)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, states, actions, discounted_rewards, sig):\n",
    "        states_tf = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        discounted_rewards_tf = tf.convert_to_tensor(discounted_rewards, dtype=tf.float32)\n",
    "        # Returns the mu for the distribution that we will sample from, want it to be between \n",
    "        # env.action_space.high and env.action_space.low\n",
    "        net_outputs = self.model(states_tf)\n",
    "        #print(\"net_output: \",net_outputs)\n",
    "        mu=2 * net_outputs\n",
    "        # get the policy pi(a,s)\n",
    "        # mu, sig = self.get_dist_params(net_outputs)\n",
    "        pi_a_s = self.gaussian_policy(mu=mu, actions=actions, sig=sig)\n",
    "        #print(\"pi(a|s): \", pi_a_s)\n",
    "        neg_log_probability = -1*tf.math.log(pi_a_s)\n",
    "        loss = tf.reduce_sum(tf.squeeze(neg_log_probability)*discounted_rewards_tf)\n",
    "        return loss\n",
    "    \n",
    "    def train_step(self, states, actions, discounted_rewards, sig, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss(states, actions, discounted_rewards, sig)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "daf7b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initiate model\n",
    "\"\"\"\n",
    "\n",
    "input_layer = Input(shape=state_shape)\n",
    "norm = LayerNormalization()(input_layer)\n",
    "layer_1 = Dense(64, activation='relu')(input_layer)\n",
    "layer_2 = Dense(64, activation='relu')(layer_1)\n",
    "output_layer = Dense(1, activation='tanh')(layer_2)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2903bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CPG = Continuous_PG(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "16621789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_episode(model, env, sig):\n",
    "    episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "    done=False\n",
    "    Return=0\n",
    "    state=env.reset()\n",
    "    while not done:\n",
    "        episode_states.append(list(state))\n",
    "        state = state.reshape((1, env.observation_space.shape[0]))\n",
    "        net_out = CPG.model(state)\n",
    "        mu = CPG.get_dist_params(net_outs=net_out)\n",
    "        a = tf.clip_by_value(np.random.normal(loc=mu, scale=sig),-1.999,1.999)\n",
    "        episode_actions.append(a.numpy()[0])\n",
    "        next_state, reward, done, info = env.step(a.numpy()[0])\n",
    "        Return+=reward\n",
    "        episode_rewards.append(reward)\n",
    "        state=next_state\n",
    "    rewards_to_go = CPG.discount_and_normalize_rewards(episode_rewards)\n",
    "    return episode_states, episode_actions, rewards_to_go, Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "75193e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:0, Return:-1209.2587182346128\n",
      "Iteration:25, Return:-1629.8311411724974\n",
      "Iteration:50, Return:-1627.6478298510483\n",
      "Iteration:75, Return:-1525.7003551914863\n",
      "Iteration:100, Return:-1079.4923287371003\n",
      "Iteration:125, Return:-1237.0916937610377\n",
      "Iteration:150, Return:-1524.674005493792\n",
      "Iteration:175, Return:-1636.35547018304\n",
      "Iteration:200, Return:-1616.7287284656618\n",
      "Iteration:225, Return:-1630.9089898515374\n",
      "Iteration:250, Return:-1460.1463650081525\n",
      "Iteration:275, Return:-1019.7926002174449\n",
      "Iteration:300, Return:-1026.1138916360792\n",
      "Iteration:325, Return:-1390.6177394728124\n",
      "Iteration:350, Return:-972.6096477636665\n",
      "Iteration:375, Return:-1535.5264592131944\n",
      "Iteration:400, Return:-1527.437886478184\n",
      "Iteration:425, Return:-1474.6928177298068\n",
      "Iteration:450, Return:-1521.7085986694233\n",
      "Iteration:475, Return:-1625.1835004087466\n",
      "Iteration:500, Return:-1262.8471213437533\n",
      "Iteration:525, Return:-1635.3474386839493\n",
      "Iteration:550, Return:-1588.9075051039088\n",
      "Iteration:575, Return:-1529.459409020391\n",
      "Iteration:600, Return:-1606.143950021958\n",
      "Iteration:625, Return:-942.5490572500478\n",
      "Iteration:650, Return:-1526.9029100521655\n",
      "Iteration:675, Return:-1127.3199903366622\n",
      "Iteration:700, Return:-1379.421381788557\n",
      "Iteration:725, Return:-1639.0052384349356\n",
      "Iteration:750, Return:-1529.427154758904\n",
      "Iteration:775, Return:-1530.7698993492018\n",
      "Iteration:800, Return:-1406.9844744394495\n",
      "Iteration:825, Return:-1492.1816564010446\n",
      "Iteration:850, Return:-1622.0401632973544\n",
      "Iteration:875, Return:-1284.320102306129\n",
      "Iteration:900, Return:-1196.164030137244\n",
      "Iteration:925, Return:-1526.136054678719\n",
      "Iteration:950, Return:-1533.3958494977965\n",
      "Iteration:975, Return:-1429.1535922363873\n",
      "Iteration:1000, Return:-1282.7259206527513\n",
      "Iteration:1025, Return:-1514.2568122984321\n",
      "Iteration:1050, Return:-1418.2006583568657\n",
      "Iteration:1075, Return:-1117.935217240627\n",
      "Iteration:1100, Return:-1529.069276311971\n",
      "Iteration:1125, Return:-1259.516943640912\n",
      "Iteration:1150, Return:-1642.963495893391\n",
      "Iteration:1175, Return:-1158.3859527947093\n",
      "Iteration:1200, Return:-1525.1410865685218\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8t/mf9ym9cd7rg3wmcx5k_v_5mh0000gp/T/ipykernel_45698/1393426792.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mepisode_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_to_go\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReturn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msingle_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mreturn_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mReturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mCPG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_to_go\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/8t/mf9ym9cd7rg3wmcx5k_v_5mh0000gp/T/ipykernel_45698/1038254534.py\u001b[0m in \u001b[0;36msingle_episode\u001b[0;34m(model, env, sig)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnet_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCPG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCPG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dist_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_outs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mepisode_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/MADDPG/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/MADDPG/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py\u001b[0m in \u001b[0;36mclip_by_value\u001b[0;34m(t, clip_value_min, clip_value_max, name)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mt_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_value_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexedSlices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/MADDPG/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1158\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mother\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrepresent\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \"\"\"\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/MADDPG/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mx_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx_dim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iterations = 10000\n",
    "return_array = np.zeros(num_iterations)\n",
    "returns_benchmark=-5000\n",
    "sig=0.3\n",
    "decay=0.9995\n",
    "for i in range(num_iterations):\n",
    "    sigma = max(sig*decay, 0.05)\n",
    "    episode_states, episode_actions, rewards_to_go, Return= single_episode(CPG.model, env, sig)\n",
    "    return_array[i]=Return\n",
    "    CPG.train_step(episode_states, episode_actions, rewards_to_go, sigma, optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
    "    if Return > returns_benchmark:\n",
    "        best_model=CPG.model\n",
    "    if i%25==0:\n",
    "        print(\"Iteration:{}, Return:{}\".format(i, Return))\n",
    "#         CPG.model.save(\"CPGAgent-v0-episode{}\".format(i))\n",
    "best_model.save(\"CPGAgent-v0-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5023ab0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc2e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
